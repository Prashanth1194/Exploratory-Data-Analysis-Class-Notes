---
title: "In-class 2/18/19"
author: "S670"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE)
```

```{r, message = FALSE}
library(tidyverse)
cb_palette = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

## Logistic regression, plus dealing with messy data

**Recommended reading: Gelman & Hill pp. 79--86.** For a more detailed and more traditional treatment, try Agresti, Categorical Data Analysis, ch. 5 & 6 (3rd edition.)

### ANES

Download the ANES Time Series Cumulative Data File, containing data from 1948 to 2012:

http://www.electionstudies.org/studypages/download/datacenter_all_datasets.php

The raw data is just an astonishing mess, so get the Stata file. The `rio` library is a good choice for reading exotic data formats into R (though there are several other packages that do the same thing):

```{r logit1}
# install.packages("rio")
library(rio)
ANES = import("anes_timeseries_cdf.dta")
```

What do we care about? Suppose we care about income. Looking at the codebook for the study, the relevant variable is `VCF0114`.

```{r}
income = ANES$VCF0114
summary(income)
```

Income is measured on a scale from 1 to 5:

1:  0 to 16 percentile

2:  17 to 33 percentile

3:  34 to 67 percentile

4:  68 to 95 percentile

5:  96 to 100 percentile

(The zeroes and NA's are missing values.) This allows comparability between years. This is really an ordinal variable but we might find some advantages in treating it as quantitative.

Next we need the year for each observation. This is `VCF0004`.

```{r}
year = ANES$VCF0004
summary(year)
```

Now we need a vote variable. In order to do logistic regression, we need a binary variable. The variable `VCF0704a` gives the *two-party* Presidential vote: that is, those who voted for third-parties or didn't vote should be missing values.

```{r}
vote = ANES$VCF0704a
summary(vote)
```

The coding is that "1" means the Democrat, "2" means the Republican, and "0" or "NA" means some other outcome. We want everything to be coded as 0, 1, or NA. First, change the zeroes to NA's:

```{r}
vote[vote == 0] = NA
summary(vote)
```

Now suppose we make 1 represent Republican and 0 represent Democrat. This just requires subtracting 1:

```{r}
vote = vote - 1
summary(vote)
```

The variable really represents a two-party vote for a Republican now, so for clarity let's just rename it as such.

```{r}
Republican = vote
```

Let's also include one other variable called `survey.weights` that we'll end up using later on.

```{r}
survey.weights = ANES$VCF0009z
summary(survey.weights)
```

Now let's put our four variables into a data frame.

```{r}
ANES.df = data.frame(year, income, Republican, survey.weights)
```

Finally, I only want data for Presidential election years (years divisible by 4.) I'll use the `filter()` function in `dplyr`:

```{r}
ANES.df = filter(ANES.df, year %in% seq(1948, 2012, 4))
summary(ANES.df)
```

### The 1992 election

Let's start by picking out one year and looking at the relationship between income and vote. Let's choose 1992, which pitted incumbent Republican George H.W. Bush against Democratic challenger Bill Clinton.

```{r}
ANES1992 = subset(ANES.df, year == 1992)
summary(ANES1992)
```

The summary is a bit indirect for the `Republican` variable. A trick to get a more informative summary is to (temporarily) treat it as a factor variable.

```{r}
summary(factor(ANES1992$Republican))
```

### Logistic regression with one predictor

Let's now look at the relationship between income and vote in 1992. If we want to look at a scatterplot, we need to jitter it, as there are only five or six $x$-values and two $y$-values.

```{r, message = FALSE}
ggplot(ANES1992, aes(x=income, y=Republican)) + geom_jitter(height=0.1, width=0.25)
```

We can also summarize the data quantitatively:

```{r}
aggregate(Republican ~ income, mean, data = ANES1992)
```

This gives the proportion (out of major party voters) who voted for Bush for each income group. Aside from group zero, which represents missing values of income, we see a strictly increasing pattern. How do we model this? Three options (not the only three) include:

1. Linear regression with income as a numerical predictor.
2. Logistic regression with income as a numerical predictor.
3. Regression with income as a categorical (factor) predictor. (In this linear and logistic give identical predictions.)

Method 1 is the easiest to interpret: we get a slope that directly tells us the change in model probability of voting Republican as income goes up one category. However, linear regression for binary responses has both technical and social limitations. The technical limitation is that it only works well when probability are far from 0 and 1. Otherwise, if $x$ is unbounded, you can end up with negative probabilities or probabilities greater than 1. The social limitation is that about two-thirds of statistics professors will never speak to you again if they see you doing linear regression on binary data. While this may be considered a positive by some, it is not really feasible for the author, so I do not pursue it here.

Method 3 isn't really a model at all: it just returns the proportion within each category who voted for Bush, the same as our `aggregate()` call gave us above. There's something to be said for not fitting restrictive models when you don't have to. However, if our goal is to fit more complex models or make comparisons between different sets of data, as it eventually will be, then some degree of simplification may be useful to understand the patterns in the data. Or we might fit a simplifying model first, then go back and look at the data in more detail and see if there are any important features our model missed. That will be our basic approach here.

Method 2, logistic regression, should work well. It does require treating a predictor that isn't *inherently* a numeric variable as numeric, and requires a parametric form (effects are linear on a logit scale.) However, most of the time, doing this is reasonable as long as the pattern of the probability with $x$ is monotonic and as long as predictive accuracy is not the sole goal. (If the pattern was non-monotonic, a nonparametric method like a generalized additive model (GAM) would be a better modeling choice. If predictive accuracy is the sole goal then it increasingly seems like you have to learn neural networks, which I haven't.)

We fit such a logistic regression using `income` as a quantitative variable and omitting missing values. Logistic regression is a special case of a GLM, so we use the `glm()` function; specifying a binomial family fits a logistic regression by default. Firstly, we can just add the fitted curve to the jittered plot:

```{r, message = FALSE}
ANES1992 = subset(ANES1992, income > 0)
ggplot(ANES1992, aes(x=income, y=Republican)) + geom_jitter(height=0.1, width=0.25) + geom_smooth(method = "glm", method.args = list(family = "binomial"))
```

We can also fit it explicitly:

```{r}
Bush.logit = glm(Republican ~ income, family = binomial, data = ANES1992)
summary(Bush.logit)
```

The summary gives a lot of information; we'll focus on the coefficients. The summary tells us that

$$
\textrm{logit[P(Bush)]} = -1.27 + 0.298 \times \textrm{income}
$$

where the logit function is

$$
\textrm{logit}(x) = \log_e\frac{x}{1-x}.
$$

To find P(Bush), we invert the logit:

$$
\textrm{P(Bush)} = \frac{e^y}{1 + e^y}
$$
where

$$
y = \textrm{logit[P(Bush)]}.
$$

For a quick and dirty interpretation, the "divide by 4" rule is useful: the maximum change in probability associated with a one unit change in $x$ is the coefficient of $x$ divided by four. So going one income group changes the model probability by up to about 7.5%. This looks like it's about the increase in the curve from income group 4 to group 5.

```{r}
library(boot)
inv.logit(-1.27 + 0.298 * 4)
inv.logit(-1.27 + 0.298 * 5)
```

### Weighted regression

For various reasons like design and nonresponse bias, modern survey results are rarely a true simple random sample from the population. To adjust for groups being underrepresents or overrepresented in a sample, surveys results are **weighted**. In particular, the ANES variable `VCF0009z` contains weights to make the sample resemble the demographics of the Current Population Survey. (Note that this doesn't remove all biases, e.g. it doesn't account for people lying to you about whether they voted, so further adjustments using a voter file would improve things further.)

Using weights in logistic (and linear) regression in R is easy: just use the `weights` argument. Technically once we have weights we're no longer fitting a binomial, so use `family = quasibinomial` (this actually doesn't make a numerical difference here, but it doesn't give a warning message.)

```{r}
Bush.weighted.logit = glm(Republican ~ income, family = quasibinomial, weights = survey.weights, data = ANES1992)
summary(Bush.weighted.logit)
```

The weights only make a small difference to the coefficients. Does this make much difference to the fit? We write a function to describe the fit:

```{r}
our.logit = function(x){
  coe = coef(Bush.weighted.logit)
  y = coe[1] + coe[2] * x
  return(exp(y) / (1 + exp(y)))
}
```

Now plot the unweighted and weighted fits.

```{r}
ggplot(ANES1992, aes(x=income, y=Republican)) + geom_jitter(height=0.1, width=0.25) + geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "blue") + stat_function(fun = our.logit, color = "orange")
```

The weighted and unweighted fits are nearly indistinguishable. This is quite often the case when creating regression models with high-quality data. (On the other hand, if you're just doing simple summaries of the data, or if the data is from a low-quality source such as a random digit dial poll, weights often make a big difference.) If someone has gone to the trouble of finding weights for you, they're easy to use, so you should use them.

### Fitting a series of regressions

We're not just interested in 1992. We want to know the relationship between income and vote for every Presidential election we have data for -- is the relationship similar for every election, or are some elections different? Has there been a consistent change over time?

In programming terms, the easiest way to fit the same model on many subsets of the data is to write a function that subsets the data and fits the model, then to write a `for` loop to fit the model for each subset. If you're literally going to run out of RAM, there are much more computationally efficient approaches, but otherwise more efficiency usually isn't worth the effort.

Here's a function to fit our weighted logistic regression of vote on income for any given year.

```{r}
logit.ANES.subset = function(my.year, data){
  newdata = subset(data, year == my.year)
  newdata = subset(newdata, income > 0)
  model = glm(Republican ~ income, family = quasibinomial, weights = survey.weights, data = newdata)
  output = c(my.year, summary(model)$coef[2,1:2])
  return(output)
}
```

The function returns the year, the model's coefficient for income, and the standard error of that coefficient. We shouldn't take the standard error too literally, because we haven't accounted for the complex design of the ANES surveys -- if you really care about getting these right, take a sampling course. 

Let's test the function out on Bush-Clinton.

```{r}
logit.ANES.subset(my.year = 1992, data = ANES.df)
```

The "estimate" is the same as the weighted regression we fitted above, so it seems the function is working fine. Now we want to run the function for every Presidential election year from 1948 to 2012.

```{r}
years = seq(1948, 2012, 4)
n = length(years)
income.by.year = data.frame(year = rep(NA, n), income.coef = rep(NA, n), income.se = rep(NA, n))
for (J in 1:n){
  my.year = years[J]
  income.by.year[J,] = logit.ANES.subset(my.year = my.year, data = ANES.df)
}
```

We'll display the results using `ggplot`. The nifty `geom_pointrange()` lets us add one standard error bounds. Again, we shouldn't take these too literally, just use them to get a ballpark idea of uncertainty.

```{r}
gg = ggplot(income.by.year, aes(x = year, y = income.coef, ymin = income.coef - income.se, ymax = income.coef + income.se))
gg + geom_pointrange() + geom_smooth(method.args = list(family = "symmetric")) + ylab("Coefficient of income in weighted linear model")
```

The income coefficient is positive for every election, meaning richer people were more likely to vote Republican every time (though 1960 was close.) The general trend was an increase in the income coefficient from 1952 to 1984, then a leveling-off. There was a huge drop from 1948 to 1952; unfortunately we don't have data from before 1948 to know if the election was typical. Otherwise there are a couple of elections outside the confidence band: 1964 (Johnson over Goldwater) and 2008 (Obama over McCain.)

### Less modeling, more detail

In our regressions, we treated `income` as a quantitative variable. A simpler approach would be to treat it as a factor, and simply track the weighted proportion of each income group that (two-party) voted Republican by year. Again, this is easiest to program (if inefficient) using a `for` loop.

To find weighted means, I used use `weighted.mean()` in conjunction with `summarise()` in `dplyr`. Let's first work out how to do it for the 1992 data.

```{r}
summarise(group_by(ANES1992, income), weighted.mean(Republican, w = survey.weights, na.rm = TRUE))
```

Now we do the same thing to the bigger data set, this time grouping by both income and year, then removing the "0" income category:

```{r}
income.prop = summarise(group_by(ANES.df, income, year), weighted.mean(Republican, w = survey.weights, na.rm = TRUE))
names(income.prop) = c("income", "year", "prop.Republican")
income.prop = income.prop[income.prop$income > 0,]
```

Plot the results:

```{r}
gg = ggplot(income.prop, aes(x = year, y = prop.Republican, group = income, color = factor(income))) + geom_line() + scale_color_manual(values = cb_palette, labels = c("0 to 16", "17 to 33", "34 to 67", "68 to 95", "96 to 100"))
gg + ylab("Proportion of two-party vote for Republican") + labs(color = "Income percentile")
```

We now have a bit more detail on the trends and the aberrant results.

- The top income group is reliably the most Republican, but the bottom income group isn't always the most Democratic (although it was in the middle part of the time period.)
- In 1948 there were pretty big differences between income groups, but in the 1950s the differences between all groups except the richest were small. It's guess work whether 1948 was an aberration or whether the small income differences from 1952 to 1968 were historical unusual (though I suspect it's the latter.)
- The big coefficient for 1964 (compared to the elections before and after) might be in part an artifact of the logit scale.
- In 2008 there really was a big difference between income group, which is likely attributable to the financial crisis.

We can also draw lines to connect income groups by year. Because there are so many different years, we'll facet them rather than color them.

```{r}
ggplot(income.prop, aes(x = income, y = prop.Republican)) + geom_line() + facet_wrap(~year, ncol=5) + ylab("Proportion of two-party vote for Republican")
```

This yields less insight, but still has interesting features: notably the big magnitude of the uptick in Republicanism for the highest income group for almost every year. (It would be interesting to check if this continued to hold in 2016.)

### Data summaries vs. models

Both data summaries (like our last plot) and models (like our logistic regressions) have their uses.

- Data summaries require fewer assumptions, and often give you a fuller picture than a model. However, they can be noisy or just too complicated to easily get a grip on. 

- Models require assumptions, so in addition to being reductive, there's more potential for things to go horribly wrong. However, models can be a easy way into the data: everything gets summarized in a couple of parameters, and you can put your effort into understanding what those parameters really mean. Furthermore, complexity can easily be added to models -- for example, it's easy to build a logistic regression model with multiple predictors (as we'll see in the next set of notes.)

In practice, going back and forth between models and data summaries, as we did here, is often very useful in exploratory work. Models can usefully simplify the data so you can get the big picture. Then you can look a fuller data summary and bear in results that the big picture doesn't explain.



